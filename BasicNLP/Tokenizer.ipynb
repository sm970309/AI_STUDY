{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어휘 집합 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ratsnlp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached ratsnlp-1.0.52-py3-none-any.whl (42 kB)\n",
      "Collecting flask>=1.1.4\n",
      "  Using cached Flask-2.2.3-py3-none-any.whl (101 kB)\n",
      "Collecting pytorch-lightning==1.6.1\n",
      "  Using cached pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n",
      "Collecting transformers==4.10.0\n",
      "  Using cached transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
      "Collecting Korpora>=0.2.0\n",
      "  Using cached Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting flask-ngrok>=0.0.25\n",
      "  Using cached flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
      "Collecting flask-cors>=3.0.10\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=8.0 in c:\\conda\\lib\\site-packages (from flask>=1.1.4->ratsnlp) (8.1.3)\n",
      "Collecting Jinja2>=3.0\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0; python_version < \"3.10\" in c:\\conda\\lib\\site-packages (from flask>=1.1.4->ratsnlp) (4.8.2)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\conda\\lib\\site-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.50.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\conda\\lib\\site-packages (from pytorch-lightning==1.6.1->ratsnlp) (20.4)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\conda\\lib\\site-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.7.0)\n",
      "Requirement already satisfied: torch>=1.8.* in c:\\conda\\lib\\site-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.13.0)\n",
      "Collecting typing-extensions>=4.0.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Using cached torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\conda\\lib\\site-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.19.2)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Using cached fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Collecting PyYAML>=5.4\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting pyDeprecate<0.4.0,>=0.3.1\n",
      "  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\conda\\lib\\site-packages (from transformers==4.10.0->ratsnlp) (2020.10.15)\n",
      "Requirement already satisfied: requests in c:\\conda\\lib\\site-packages (from transformers==4.10.0->ratsnlp) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\conda\\lib\\site-packages (from transformers==4.10.0->ratsnlp) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Processing c:\\users\\고성민\\appdata\\local\\pip\\cache\\wheels\\82\\ab\\9b\\c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\\sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in c:\\conda\\lib\\site-packages (from transformers==4.10.0->ratsnlp) (0.11.1)\n",
      "Requirement already satisfied: xlrd>=1.2.0 in c:\\conda\\lib\\site-packages (from Korpora>=0.2.0->ratsnlp) (1.2.0)\n",
      "Collecting dataclasses>=0.6\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: Six in c:\\conda\\lib\\site-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\conda\\lib\\site-packages (from click>=8.0->flask>=1.1.4->ratsnlp) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\conda\\lib\\site-packages (from Jinja2>=3.0->flask>=1.1.4->ratsnlp) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\conda\\lib\\site-packages (from importlib-metadata>=3.6.0; python_version < \"3.10\"->flask>=1.1.4->ratsnlp) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\conda\\lib\\site-packages (from packaging>=17.0->pytorch-lightning==1.6.1->ratsnlp) (2.4.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.35.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (50.3.1.post20201107)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.3.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\conda\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.19.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"\n",
      "  Using cached aiohttp-3.8.4-cp38-cp38-win_amd64.whl (324 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\conda\\lib\\site-packages (from requests->transformers==4.10.0->ratsnlp) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\conda\\lib\\site-packages (from requests->transformers==4.10.0->ratsnlp) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\conda\\lib\\site-packages (from requests->transformers==4.10.0->ratsnlp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\conda\\lib\\site-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n",
      "Requirement already satisfied: joblib in c:\\conda\\lib\\site-packages (from sacremoses->transformers==4.10.0->ratsnlp) (0.17.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\conda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.2.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\conda\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (20.3.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0\n",
      "  Using cached charset_normalizer-3.1.0-cp38-cp38-win_amd64.whl (96 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.2-cp38-cp38-win_amd64.whl (56 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp38-cp38-win_amd64.whl (34 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\conda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\conda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.8)\n",
      "Installing collected packages: Jinja2, itsdangerous, Werkzeug, flask, typing-extensions, torchmetrics, charset-normalizer, async-timeout, multidict, yarl, frozenlist, aiosignal, aiohttp, fsspec, PyYAML, pyDeprecate, pytorch-lightning, tokenizers, sacremoses, transformers, dataclasses, Korpora, flask-ngrok, flask-cors, ratsnlp\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.2\n",
      "    Uninstalling Jinja2-2.11.2:\n",
      "      Successfully uninstalled Jinja2-2.11.2\n",
      "  Attempting uninstall: itsdangerous\n",
      "    Found existing installation: itsdangerous 1.1.0\n",
      "    Uninstalling itsdangerous-1.1.0:\n",
      "      Successfully uninstalled itsdangerous-1.1.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 1.1.2\n",
      "    Uninstalling Flask-1.1.2:\n",
      "      Successfully uninstalled Flask-1.1.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.8.3\n",
      "    Uninstalling fsspec-0.8.3:\n",
      "      Successfully uninstalled fsspec-0.8.3\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Korpora\n",
      "  Using cached Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\conda\\lib\\site-packages (from Korpora) (4.50.2)\n",
      "Requirement already satisfied: xlrd>=1.2.0 in c:\\conda\\lib\\site-packages (from Korpora) (1.2.0)\n",
      "Collecting dataclasses>=0.6\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\conda\\lib\\site-packages (from Korpora) (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\conda\\lib\\site-packages (from Korpora) (1.19.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\conda\\lib\\site-packages (from requests>=2.20.0->Korpora) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\conda\\lib\\site-packages (from requests>=2.20.0->Korpora) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\conda\\lib\\site-packages (from requests>=2.20.0->Korpora) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\conda\\lib\\site-packages (from requests>=2.20.0->Korpora) (1.25.11)\n",
      "Installing collected packages: dataclasses, Korpora\n",
      "Successfully installed Korpora-0.2.0 dataclasses-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install ratsnlp\n",
    "!pip install Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 77.2MB/s]                                                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 50.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "nsmc = Korpora.load('nsmc',force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def write_lines(path,lines):\n",
    "    with open(path,'w',encoding='utf-8') as f:\n",
    "        for line in lines:\n",
    "            f.write(f'{line}\\n')\n",
    "write_lines('train.txt',nsmc.train.get_all_texts())\n",
    "write_lines('test.txt',nsmc.test.get_all_texts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BPE 어휘 집합 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlpbook/bbpe\\\\vocab.json', 'nlpbook/bbpe\\\\merges.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "os.makedirs('nlpbook/bbpe',exist_ok=True)\n",
    "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
    "bytebpe_tokenizer.train(\n",
    "    files = ['train.txt','test.txt'],\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"[PAD]\"]\n",
    ")\n",
    "bytebpe_tokenizer.save_model('nlpbook/bbpe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 토크나이저 선언 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained('nlpbook/bbpe')\n",
    "tokenizer_gpt.pad_token='[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 문장\n",
    "sentences = []\n",
    "with open('test.txt','r',encoding='utf-8') as f:\n",
    "    for text in f.readlines()[100:103]:\n",
    "        sentences.append(text)\n",
    "batch_inputs = tokenizer_gpt(\n",
    "    sentences,\n",
    "    padding=\"max_length\",\n",
    "    max_length=12,\n",
    "    truncation=True # 문장 잘림 허용 옵션\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 실행 결과\n",
    "> *두가지 입력값이 만들어짐*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_ids -> 토큰화 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[652, 6353, 1433, 7738, 2033, 3856, 9560, 1649, 14, 199, 0, 0],\n",
       " [1717, 456, 6316, 1484, 199, 0, 0, 0, 0, 0, 0, 0],\n",
       " [5999, 337, 534, 3306, 14, 302, 9936, 264, 4480, 311, 634, 432]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention_mask -> 일반토큰과 패딩토큰 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert에서 사용하는 WordPieceTokenizer 어휘집합 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlpbook/wordpiece\\\\vocab.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "os.makedirs('nlpbook/wordpiece',exist_ok=True)\n",
    "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "wordpiece_tokenizer.train(\n",
    "    files=['train.txt','test.txt'],\n",
    "    vocab_size=10000\n",
    ")\n",
    "wordpiece_tokenizer.save_model('nlpbook/wordpiece')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 토크나이저 선언 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('nlpbook/wordpiece/',do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = tokenizer_bert(\n",
    "    sentences,\n",
    "    padding='max_length',\n",
    "    max_length=12,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 실행 결과\n",
    "> *세가지 입력값이 만들어짐*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_ids -> 토큰화 결과(시작 2[CLS], 끝3[SEP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2741, 1025, 444, 9176, 2589, 3216, 8492, 2421, 16, 3, 0],\n",
       " [2, 2087, 4893, 1073, 2325, 3, 0, 0, 0, 0, 0, 0],\n",
       " [2, 3366, 16, 16, 16, 1988, 3360, 16, 1979, 9086, 16, 3]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention_mask -> 일반토큰과 패딩토큰 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = tokenizer_bert(\n",
    "    sentences,\n",
    "    padding='max_length',\n",
    "    max_length=12,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2741, 1025, 444, 9176, 2589, 3216, 8492, 2421, 16, 3, 0],\n",
       " [2, 2087, 4893, 1073, 2325, 3, 0, 0, 0, 0, 0, 0],\n",
       " [2, 3366, 16, 16, 16, 1988, 3360, 16, 1979, 9086, 16, 3]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세그먼트 정보(첫번째 문장인지, 두번째 문장인지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['token_type_ids']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
